{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4AbxwEfjMpKtWRQhZ9mfn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":119,"metadata":{"id":"ezbATuFGzowE","executionInfo":{"status":"ok","timestamp":1700515574828,"user_tz":-60,"elapsed":323,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import time\n","#import networks\n","\n","\n","\n","class Transformer():\n","  \"\"\"\n","  An abstract father class for the Transformers.\n","  Wasn't necessary but I think it makes the code tidy\n","  and raises exceptions if we forget to implement something.\n","\n","  Each transformer is basically just a container for the matrices needed\n","  for backsubstitution. Making them nn.Modules as well doesn't seem\n","  to me as the most intuitive choice, but that can be changed\n","  \"\"\"\n","\n","  def __init__(self):\n","    raise Exception(\"__init__ method needs to be implemented for each Transformer subclass\")\n","\n","  def create_backsub_matrices(self, clb: torch.Tensor, cub: torch.Tensor):\n","    \"\"\"\n","      this second method is needed since some tranformers, like the leaky relu one,\n","      needs to know the actual concrete lowe and upper bounds to compute\n","      the matrices needed during backsobstitution, in addition to the layer's\n","      attribute\n","    \"\"\"\n","    raise Exception(\"create_backsub_matrices method needs to be implemented for each Transformer subclass\")\n","\n","\n","\n","class LinearTransformer(Transformer):\n","\n","  def __init__(self, layer : nn.Linear):\n","    self.weight = layer.weight.clone().detach_()\n","    self.bias = layer.bias.clone().detach_()\n","\n","    #this two are to be used when they are the first to be multiplied in\n","    #backsub, that is, when the layer they are associated with is the current\n","    #layer we backsubstitute to\n","    self.weight_cat_bias_lb = torch.cat([self.weight, torch.unsqueeze(self.bias, dim = 1)], dim = 1)\n","    self.weight_cat_bias_ub = self.weight_cat_bias_lb\n","\n","\n","    row = torch.zeros(self.weight_cat_bias_lb.shape[1])\n","    row[-1] = 1\n","    row = torch.unsqueeze(row, dim = 0)\n","\n","    #while this to are to be used when they are not associated\n","    #with the current layer, and so we add the 0000...01 row\n","    self.weight_cat_bias_backsub_lb = torch.cat([self.weight_cat_bias_lb, row], dim = 0)\n","    self.weight_cat_bias_backsub_ub = self.weight_cat_bias_backsub_lb\n","\n","  def create_backsub_matrices(self, clb: torch.Tensor, cub: torch.Tensor):\n","    pass\n","\n","\n","class Conv2dTransformer(Transformer):\n","\n","  def __init__(self, layer : nn.Conv2d):\n","    raise Exception(\"The Conv2dTransformer has not been implemented yet\")\n","\n","\n","  def create_backsub_matrices(self, clb: torch.Tensor, cub: torch.Tensor):\n","    pass\n","\n","\n","class BasicReluTransformer(Transformer):\n","\n","  def __init__(self, layer: nn.ReLU):\n","    self.weight_cat_bias_lb = None\n","    self.weight_cat_bias_ub = None\n","    self.weight_cat_bias_backsub_lb = None\n","    self.weight_cat_bias_backsub_ub = None\n","\n","  def compute_lambda_and_intercept(lb: float, ub: float):\n","    \"\"\"\n","      this is a utility method that computes the slope (lambda) and the\n","      intercept of the line that constitute the upper constraint when the relu\n","      is crossing\n","    \"\"\"\n","    lambda_ = ub/(ub-lb)\n","    return lambda_, -(lambda_ * lb)\n","\n","  def create_backsub_matrices(self, clb: torch.Tensor, cub: torch.Tensor):\n","\n","\n","    diag_entries_lb = torch.zeros_like(clb)\n","    diag_entries_ub = torch.zeros_like(cub)\n","    bias_ub = torch.zeros_like(cub)\n","    bias_lb = torch.zeros_like(clb)\n","\n","    for i in range(len(clb)):\n","      if cub[i] <= 0:\n","        diag_entries_lb[i] = 0\n","        diag_entries_ub[i] = 0\n","      elif clb[i] >= 0 :\n","        diag_entries_lb[i] = 1\n","        diag_entries_ub[i] = 1\n","      else:\n","        diag_entries_lb[i] = 0\n","        lambda_ , intercept = BasicReluTransformer.compute_lambda_and_intercept(clb[i], cub[i])\n","        diag_entries_ub[i] = lambda_\n","        bias_ub[i] = intercept\n","\n","    self.weight_lb = torch.diag(diag_entries_lb)\n","    self.weight_ub = torch.diag(diag_entries_ub)\n","\n","    self.weight_cat_bias_lb = torch.cat([self.weight_lb, torch.unsqueeze(bias_lb, dim = 1)], dim = 1)\n","    self.weight_cat_bias_ub = torch.cat([self.weight_ub, torch.unsqueeze(bias_ub, dim = 1)], dim = 1)\n","\n","\n","    row = torch.zeros(self.weight_cat_bias_lb.shape[1])\n","    row[-1] = 1\n","    row = torch.unsqueeze(row, dim = 0)\n","    self.weight_cat_bias_backsub_lb = torch.cat([self.weight_cat_bias_lb, row], dim = 0)\n","    self.weight_cat_bias_backsub_ub = torch.cat([self.weight_cat_bias_ub, row], dim = 0)\n","\n","\n","\n","class BasicLeakyReluTransformer(Transformer):\n","\n","  def __init__(self, layer : nn.LeakyReLU):\n","\n","    self.negative_slope = layer.negative_slope\n","    self.weight_cat_bias_lb = None\n","    self.weight_cat_bias_ub = None\n","    self.weight_cat_bias_backsub_lb = None\n","    self.weight_cat_bias_backsub_ub = None\n","\n","  def compute_lambda_and_intercept(self, lb: float, ub: float):\n","    \"\"\"\n","      this is a utility method that computes the slope (lambda) and the\n","      intercept of the line that constitute the upper constraint when the leaky\n","      relu is crossing\n","    \"\"\"\n","    lambda_ = (ub - (lb * self.negative_slope))/(ub-lb)\n","    intercept = ((self.negative_slope - 1) * ub * lb) / ( ub - lb)\n","    return lambda_, intercept\n","\n","  def create_backsub_matrices(self, clb: torch.Tensor, cub: torch.Tensor):\n","\n","\n","    diag_entries_lb = torch.zeros_like(clb)\n","    diag_entries_ub = torch.zeros_like(cub)\n","    bias_ub = torch.zeros_like(cub)\n","    bias_lb = torch.zeros_like(clb)\n","\n","    for i in range(len(clb)):\n","      if cub[i] <= 0:\n","        diag_entries_lb[i] = clb[i]*self.negative_slope\n","        diag_entries_ub[i] = cub[i]*self.negative_slope\n","      elif clb[i] >= 0 :\n","        diag_entries_lb[i] = 1\n","        diag_entries_ub[i] = 1\n","      else:\n","        diag_entries_lb[i] = clb[i]*self.negative_slope\n","        lambda_ , intercept = self.compute_lambda_and_intercept(clb[i], cub[i])\n","        diag_entries_ub[i] = lambda_\n","        bias_ub[i] = intercept\n","\n","    self.weigth_lb = torch.diag(diag_entries_lb)\n","    self.weigth_ub = torch.diag(diag_entries_ub)\n","\n","    self.weight_cat_bias_lb = torch.cat([self.weight_lb, torch.unsqueeze(bias_lb, dim = 1)], dim = 1)\n","    self.weight_cat_bias_ub = torch.cat([self.weight_ub, torch.unsqueeze(bias_ub, dim = 1)], dim = 1)\n","\n","\n","    row = torch.zeros(self.weight_cat_bias_lb.shape[1])\n","    row[-1] = 1\n","    row = torch.unsqueeze(row, dim = 0)\n","    self.weight_cat_bias_backsub_lb = torch.cat([self.weight_cat_bias_lb, row], dim = 0)\n","    self.weight_cat_bias_backsub_ub = torch.cat([self.weight_cat_bias_ub, row], dim = 0)\n","\n","\n","\n","class LeakyReluTransformer(Transformer):\n","  \"\"\"\n","    For now an early attempt at implementing a tranformer that allows us to tune\n","    the slopes (alphas) of the lower constrain lines for when the leaky relus are\n","    crossing\n","  \"\"\"\n","\n","  def __init__(self, layer : nn.LeakyReLU, width: int):\n","\n","    self.negative_slope = layer.negative_slope\n","    self.alphas = torch.zeros(width) + self.negative_slope\n","\n","\n","  def compute_lambda_and_intercept(self, lb: float, ub: float):\n","    lambda_ = (ub - (lb * self.negative_slope))/(ub-lb)\n","    intercept = ((self.negative_slope - 1) * ub * lb) / ( ub - lb)\n","    return lambda_, intercept\n","\n","  def create_backsub_matrices(self, clb: torch.Tensor, cub: torch.Tensor):\n","\n","\n","    diag_entries_lb = torch.zeros_like(clb)\n","    diag_entries_ub = torch.zeros_like(cub)\n","    bias_ub = torch.zeros_like(cub)\n","    bias_lb = torch.zeros_like(clb)\n","\n","    for i in range(len(clb)):\n","      if cub[i] <= 0:\n","        diag_entries_lb[i] = clb[i]*self.negative_slope\n","        diag_entries_ub[i] = cub[i]*self.negative_slope\n","      elif clb[i] >= 0 :\n","        diag_entries_lb[i] = 1\n","        diag_entries_ub[i] = 1\n","      else:\n","        diag_entries_lb[i] = clb[i]*self.alphas[i]\n","        lambda_ , intercept = self.compute_lambda_and_intercept(clb[i], cub[i])\n","        diag_entries_ub[i] = lambda_\n","        bias_ub[i] = intercept\n","\n","    self.weigth_lb = torch.diag(diag_entries_lb)\n","    self.weigth_ub = torch.diag(diag_entries_ub)\n","\n","    self.weight_cat_bias_lb = torch.cat([self.weight_lb, torch.unsqueeze(bias_lb, dim = 1)], dim = 1)\n","    self.weight_cat_bias_ub = torch.cat([self.weight_ub, torch.unsqueeze(bias_ub, dim = 1)], dim = 1)\n","\n","\n","    row = torch.zeros(self.weight_cat_bias_lb.shape[1])\n","    row[-1] = 1\n","    row = torch.unsqueeze(row, dim = 0)\n","    self.weight_cat_bias_backsub_lb = torch.cat([self.weight_cat_bias_lb, row], dim = 0)\n","    self.weight_cat_bias_backsub_ub = torch.cat([self.weight_cat_bias_ub, row], dim = 0)\n","\n","\n","\n","class VerificationTransformer(Transformer):\n","  \"\"\"\n","    The final transformer of the verifying network. It computes the 9\n","    differences of the logits corresponding to the true label of the sample\n","    and all other 9 logits\n","  \"\"\"\n","\n","  def __init__(self):\n","\n","    self.weight_cat_bias_lb = torch.Tensor([[1.,-1., 0.]])\n","    self.weight_cat_bias_ub = self.weight_cat_bias_lb\n","\n","\n","    row = torch.zeros(self.weight_cat_bias_lb.shape[1])\n","    row[-1] = 1\n","    row = torch.unsqueeze(row, dim = 0)\n","\n","    #actually don't need this, just added for \"symmetry\"\n","    self.weight_cat_bias_backsub_lb = torch.cat([self.weight_cat_bias_lb, row], dim = 0)\n","    self.weight_cat_bias_backsub_ub = self.weight_cat_bias_backsub_lb\n","\n","  def create_backsub_matrices(self, clb: torch.Tensor, cub: torch.Tensor):\n","    pass\n","\n","\n","\n","class TransformerNet(nn.Module):\n","\n","  \"\"\"\n","    The object we push the initial lb and ub through\n","  \"\"\"\n","\n","  def __init__(self, net: nn.Module, verbose = False):\n","\n","    \"\"\"\n","      This constructor scans the modules of the original network and builds a\n","      list of corresponding tansformers\n","    \"\"\"\n","    super().__init__()\n","    self.verbose = verbose\n","    self.transformers_list = []\n","    self.clb_list = []\n","    self.cub_list = []\n","    self.backsub_matrices_lb = []\n","    self.backsub_matrices_ub = []\n","\n","    if self.verbose:\n","      print(\"-\"*40 + \"__init__\"+\"-\"*40+\"\\n\")\n","\n","    #copied from the repo\n","    #net_layers = [layer for layer in net.modules() if type(layer) not in [networks.FullyConnected, networks.Conv, nn.Sequential]]\n","    net_layers = [layer for layer in net.modules() if type(layer) not in [ nn.Sequential]]\n","\n","    for i,layer in enumerate(net_layers):\n","      #if type(layer) == networks.Normalization:\n","          #pass\n","      if type(layer) == nn.Flatten:\n","          pass\n","      elif type(layer) == nn.Linear:\n","          self.transformers_list.append(LinearTransformer(layer))\n","      elif type(layer) == nn.ReLU:\n","          self.transformers_list.append(BasicReluTransformer(layer))\n","      elif type(layer) == nn.LeakyReLU:\n","          self.transformers_list.append(BasicLeakyReluTransformer(layer))\n","      elif type(layer) == nn.Conv2d:\n","          self.transformers_list.append(Conv2dTransformer(layer))\n","      else:\n","          raise Exception(\"Layer \"+ str(type(layer))+ \"has not a corresponding transformer yet\")\n","\n","\n","      if self.verbose:\n","        print(\"Transformer n. {}: \".format(i+1)\n","              + str(type(self.transformers_list[-1])))\n","        print()\n","        print(\"weight_cat_bias_lb: \")\n","        print(self.transformers_list[-1].weight_cat_bias_lb)\n","        print()\n","        print(\"weight_cat_bias_ub: \")\n","        print(self.transformers_list[-1].weight_cat_bias_ub)\n","        print()\n","        print(\"weight_cat_bias_backsub_lb: \")\n","        print(self.transformers_list[-1].weight_cat_bias_backsub_lb)\n","        print()\n","        print(\"weight_cat_bias_backsub_ub: \")\n","        print(self.transformers_list[-1].weight_cat_bias_backsub_ub)\n","        print()\n","        print(\"-\"*88+\"\\n\")\n","\n","    self.transformers_list.append(VerificationTransformer())\n","\n","    if self.verbose:\n","        print(\"Verification Transformer: \"\n","              + str(type(self.transformers_list[-1])))\n","        print()\n","        print(\"weight_cat_bias_lb: \")\n","        print(self.transformers_list[-1].weight_cat_bias_lb)\n","        print()\n","        print(\"weight_cat_bias_ub: \")\n","        print(self.transformers_list[-1].weight_cat_bias_ub)\n","        print()\n","        print(\"weight_cat_bias_backsub_lb: \")\n","        print(self.transformers_list[-1].weight_cat_bias_backsub_lb)\n","        print()\n","        print(\"weight_cat_bias_backsub_ub: \")\n","        print(self.transformers_list[-1].weight_cat_bias_backsub_ub)\n","        print()\n","        print(\"-\"*36+\"__init__ completed\"+\"-\"*36+\"\\n\\n\\n\")\n","\n","\n","\n","\n","\n","\n","  def forward(self, clb: torch.Tensor, cub: torch.Tensor):\n","\n","    \"\"\"\n","      this implementation assumes that we always want to backsobstitute to the\n","      input \"layer\", and peforms the full chain of matrix multiplications anew\n","      for each layer\n","    \"\"\"\n","    self.starting_time = time.time()\n","\n","    if self.verbose:\n","      print(\"-\"*40+\"forward\"+\"-\"*40+\"\\n\")\n","      print(\"Input lb:\")\n","      print(clb)\n","      print()\n","      print(\"Input ub:\")\n","      print(cub)\n","      print()\n","      print(\"-\"*88+\"\\n\")\n","\n","    #these list contain the concrete ub and lb for each layer\n","    #the first entry is the initial tensor of lb and ub passed as arguments\n","    #maybe we only need the concrete constrains of last layer, but\n","    #keeping everything might be useful for debugging\n","\n","    self.clb_list.append(clb)\n","    self.cub_list.append(cub)\n","\n","    #these contain the matrices necessary to perform backsub\n","    #backsub is performed by multiplying the matrices bacwards without\n","    #transposing them\n","    self.backsub_matrices_lb.append(torch.cat([clb, torch.ones(1)], dim = 0))\n","    self.backsub_matrices_ub.append(torch.cat([cub, torch.ones(1)], dim = 0))\n","\n","    if self.verbose:\n","      print(\"Layer 0 (before first layer of the network)\\n\")\n","      print(\"clb_list:\")\n","      print(self.clb_list)\n","      print()\n","      print(\"cub_list:\")\n","      print(self.cub_list)\n","      print()\n","      print(\"backsub_matrices_lb:\")\n","      print(self.backsub_matrices_lb[-1])\n","      print()\n","      print(\"backsub_matrices_ub:\")\n","      print(self.backsub_matrices_ub[-1])\n","      print()\n","      print(\"-\"*88+\"\\n\")\n","\n","\n","\n","\n","\n","    for n,transformer in enumerate(self.transformers_list):\n","\n","\n","\n","      #backsubstitution, in all its glory and splendor\n","\n","      #fetch the backsub matrices of the current transformer, the one without\n","      #the extra 00...001 row\n","      transformer.create_backsub_matrices(self.clb_list[-1], self.cub_list[-1] )\n","      weight_cat_bias_lb = transformer.weight_cat_bias_lb\n","      weight_cat_bias_ub = transformer.weight_cat_bias_ub\n","\n","      new_clb = weight_cat_bias_lb\n","\n","      new_cub = weight_cat_bias_ub\n","\n","\n","      #to the mat multiplicattion to the very beginning\n","      for i in range( len(self.backsub_matrices_lb)-1,-1, -1):\n","\n","        current_weight_cat_bias_backsub_lb = self.backsub_matrices_lb[i]\n","\n","        current_weight_cat_bias_backsub_ub = self.backsub_matrices_ub[i]\n","\n","        new_clb_positive = F.relu(new_clb)\n","\n","        new_clb_negative = F.relu(-new_clb)\n","\n","        new_cub_positive = F.relu(new_cub)\n","\n","        new_cub_negative = F.relu(-new_cub)\n","\n","        new_clb = torch.matmul(new_clb_positive, current_weight_cat_bias_backsub_lb ) - torch.matmul(new_clb_negative, current_weight_cat_bias_backsub_ub )\n","\n","        new_cub = torch.matmul(new_cub_positive, current_weight_cat_bias_backsub_ub ) - torch.matmul(new_cub_negative, current_weight_cat_bias_backsub_lb )\n","\n","\n","      #add the newly computed concrete bounds to the list\n","      self.clb_list.append(new_clb)\n","      self.cub_list.append(new_cub)\n","\n","      #add the backsub matrices to the list, this time the version enhanced with\n","      #the extra 000...0001 row\n","      weight_cat_bias_backsub_lb = transformer.weight_cat_bias_backsub_lb\n","      weight_cat_bias_backsub_ub = transformer.weight_cat_bias_backsub_ub\n","\n","      self.backsub_matrices_lb.append(weight_cat_bias_backsub_lb)\n","      self.backsub_matrices_ub.append(weight_cat_bias_backsub_ub)\n","\n","      if self.verbose:\n","        print(\"Layer {}\\n\".format(n+1))\n","        print(\"clb_list:\")\n","        print(self.clb_list)\n","        print()\n","        print(\"cub_list:\")\n","        print(self.cub_list)\n","        print()\n","        print(\"backsub_matrices_lb:\")\n","        for mat in self.backsub_matrices_lb:\n","          print(mat)\n","          print()\n","        print(\"\\n\")\n","        print(\"backsub_matrices_ub:\")\n","        for mat in self.backsub_matrices_ub:\n","          print(mat)\n","          print()\n","        print()\n","\n","        print(\"-\"*88+\"\\n\")\n","\n","\n","    if self.verbose:\n","      print(\"-\"*35+\"forward completed\"+\"-\"*35+\"\\n\")\n","      print(\"Final concrete lb:\")\n","      print(self.clb_list[-1])\n","      print()\n","      print(\"Total time: {} sec\".format(time.time()-self.starting_time))\n","      print()\n","      print(\"-\"*88+\"\\n\")\n","\n","\n","    return self.clb_list[-1], self.cub_list[-1]\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Build the toy networks in slide 22 in LECTURE4_DEEPPOLY slides"],"metadata":{"id":"4IVWjXfenJQ8"}},{"cell_type":"code","source":["#create the toy network\n","\n","\n","layer_1 = nn.Linear(2,2)\n","layer_2 = nn.ReLU()\n","layer_3 = nn.Linear(2,2)\n","layer_4 = nn.ReLU()\n","layer_5 = nn.Linear(2,2)\n"],"metadata":{"id":"P4sKhF951t1j","executionInfo":{"status":"ok","timestamp":1700515009044,"user_tz":-60,"elapsed":3,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    layer_1.weight = nn.Parameter(torch.Tensor([[1.,1.],[1.,-1.]]))\n","    layer_1.bias = nn.Parameter(torch.zeros_like(layer_1.bias))\n","    layer_3.weight = nn.Parameter(torch.Tensor([[1.,1.],[1.,-1.]]))\n","    layer_3.bias = nn.Parameter(torch.Tensor([-0.5, 0.]))\n","    layer_5.weight = nn.Parameter(torch.Tensor([[-1.,1.],[0.,1.]]))\n","    layer_5.bias = nn.Parameter(torch.Tensor([3., 0.]))\n"],"metadata":{"id":"3H5_c7Agv-Ws","executionInfo":{"status":"ok","timestamp":1700515009334,"user_tz":-60,"elapsed":5,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["net = nn.Sequential(layer_1, layer_2, layer_3, layer_4, layer_5)"],"metadata":{"id":"6eoB77JywfZP","executionInfo":{"status":"ok","timestamp":1700515009936,"user_tz":-60,"elapsed":4,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["net"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E47OQSZXx0xO","executionInfo":{"status":"ok","timestamp":1700515010218,"user_tz":-60,"elapsed":3,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}},"outputId":"c52d8df1-d729-4c37-e826-8ae375100d7e"},"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=2, out_features=2, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=2, out_features=2, bias=True)\n","  (3): ReLU()\n","  (4): Linear(in_features=2, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["net.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n56OJdD3x1sP","executionInfo":{"status":"ok","timestamp":1700515011206,"user_tz":-60,"elapsed":8,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}},"outputId":"0d2b98f9-9a7e-48da-f392-c6e3ea8b09a6"},"execution_count":113,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('0.weight',\n","              tensor([[ 1.,  1.],\n","                      [ 1., -1.]])),\n","             ('0.bias', tensor([0., 0.])),\n","             ('2.weight',\n","              tensor([[ 1.,  1.],\n","                      [ 1., -1.]])),\n","             ('2.bias', tensor([-0.5000,  0.0000])),\n","             ('4.weight',\n","              tensor([[-1.,  1.],\n","                      [ 0.,  1.]])),\n","             ('4.bias', tensor([3., 0.]))])"]},"metadata":{},"execution_count":113}]},{"cell_type":"markdown","source":["## Now let's test the TransformerNet"],"metadata":{"id":"p8tiw8TVnXjG"}},{"cell_type":"code","source":["transformerNet = TransformerNet(net, verbose = True )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yEEJU0Ux5eo","executionInfo":{"status":"ok","timestamp":1700515011900,"user_tz":-60,"elapsed":7,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}},"outputId":"67fed7cf-fadc-45da-9ac4-970a83aecace"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------__init__----------------------------------------\n","\n","Transformer n. 1: <class '__main__.LinearTransformer'>\n","\n","weight_cat_bias_lb: \n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.]])\n","\n","weight_cat_bias_ub: \n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.]])\n","\n","weight_cat_bias_backsub_lb: \n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","weight_cat_bias_backsub_ub: \n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","----------------------------------------------------------------------------------------\n","\n","Transformer n. 2: <class '__main__.BasicReluTransformer'>\n","\n","weight_cat_bias_lb: \n","None\n","\n","weight_cat_bias_ub: \n","None\n","\n","weight_cat_bias_backsub_lb: \n","None\n","\n","weight_cat_bias_backsub_ub: \n","None\n","\n","----------------------------------------------------------------------------------------\n","\n","Transformer n. 3: <class '__main__.LinearTransformer'>\n","\n","weight_cat_bias_lb: \n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000]])\n","\n","weight_cat_bias_ub: \n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000]])\n","\n","weight_cat_bias_backsub_lb: \n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","weight_cat_bias_backsub_ub: \n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","----------------------------------------------------------------------------------------\n","\n","Transformer n. 4: <class '__main__.BasicReluTransformer'>\n","\n","weight_cat_bias_lb: \n","None\n","\n","weight_cat_bias_ub: \n","None\n","\n","weight_cat_bias_backsub_lb: \n","None\n","\n","weight_cat_bias_backsub_ub: \n","None\n","\n","----------------------------------------------------------------------------------------\n","\n","Transformer n. 5: <class '__main__.LinearTransformer'>\n","\n","weight_cat_bias_lb: \n","tensor([[-1.,  1.,  3.],\n","        [ 0.,  1.,  0.]])\n","\n","weight_cat_bias_ub: \n","tensor([[-1.,  1.,  3.],\n","        [ 0.,  1.,  0.]])\n","\n","weight_cat_bias_backsub_lb: \n","tensor([[-1.,  1.,  3.],\n","        [ 0.,  1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","weight_cat_bias_backsub_ub: \n","tensor([[-1.,  1.,  3.],\n","        [ 0.,  1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","----------------------------------------------------------------------------------------\n","\n","Verification Transformer: <class '__main__.VerificationTransformer'>\n","\n","weight_cat_bias_lb: \n","tensor([[ 1., -1.,  0.]])\n","\n","weight_cat_bias_ub: \n","tensor([[ 1., -1.,  0.]])\n","\n","weight_cat_bias_backsub_lb: \n","tensor([[ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","weight_cat_bias_backsub_ub: \n","tensor([[ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","------------------------------------__init__ completed------------------------------------\n","\n","\n","\n"]}]},{"cell_type":"code","source":["#define the input tensors as in the slide\n","\n","lb = torch.Tensor([-1., -1.])\n","ub = torch.Tensor([1., 1.])"],"metadata":{"id":"TFZxu6_NCmqy","executionInfo":{"status":"ok","timestamp":1700515085043,"user_tz":-60,"elapsed":317,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["final_lb, final_ub = transformerNet(lb, ub)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4YDI_4aoDftL","executionInfo":{"status":"ok","timestamp":1700515086693,"user_tz":-60,"elapsed":427,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}},"outputId":"879c713d-7259-43fd-b417-209261545c87"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------forward----------------------------------------\n","\n","Input lb:\n","tensor([-1., -1.])\n","\n","Input ub:\n","tensor([1., 1.])\n","\n","----------------------------------------------------------------------------------------\n","\n","Layer 0 (before first layer of the network)\n","\n","clb_list:\n","[tensor([-1., -1.])]\n","\n","cub_list:\n","[tensor([1., 1.])]\n","\n","backsub_matrices_lb:\n","tensor([-1., -1.,  1.])\n","\n","backsub_matrices_ub:\n","tensor([1., 1., 1.])\n","\n","----------------------------------------------------------------------------------------\n","\n","Layer 1\n","\n","clb_list:\n","[tensor([-1., -1.]), tensor([-2., -2.])]\n","\n","cub_list:\n","[tensor([1., 1.]), tensor([2., 2.])]\n","\n","backsub_matrices_lb:\n","tensor([-1., -1.,  1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","\n","\n","backsub_matrices_ub:\n","tensor([1., 1., 1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","\n","----------------------------------------------------------------------------------------\n","\n","Layer 2\n","\n","clb_list:\n","[tensor([-1., -1.]), tensor([-2., -2.]), tensor([0., 0.])]\n","\n","cub_list:\n","[tensor([1., 1.]), tensor([2., 2.]), tensor([2., 2.])]\n","\n","backsub_matrices_lb:\n","tensor([-1., -1.,  1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 1.]])\n","\n","\n","\n","backsub_matrices_ub:\n","tensor([1., 1., 1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0.5000, 0.0000, 1.0000],\n","        [0.0000, 0.5000, 1.0000],\n","        [0.0000, 0.0000, 1.0000]])\n","\n","\n","----------------------------------------------------------------------------------------\n","\n","Layer 3\n","\n","clb_list:\n","[tensor([-1., -1.]), tensor([-2., -2.]), tensor([0., 0.]), tensor([-0.5000, -2.0000])]\n","\n","cub_list:\n","[tensor([1., 1.]), tensor([2., 2.]), tensor([2., 2.]), tensor([2.5000, 2.0000])]\n","\n","backsub_matrices_lb:\n","tensor([-1., -1.,  1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 1.]])\n","\n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","\n","\n","backsub_matrices_ub:\n","tensor([1., 1., 1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0.5000, 0.0000, 1.0000],\n","        [0.0000, 0.5000, 1.0000],\n","        [0.0000, 0.0000, 1.0000]])\n","\n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","\n","----------------------------------------------------------------------------------------\n","\n","Layer 4\n","\n","clb_list:\n","[tensor([-1., -1.]), tensor([-2., -2.]), tensor([0., 0.]), tensor([-0.5000, -2.0000]), tensor([0., 0.])]\n","\n","cub_list:\n","[tensor([1., 1.]), tensor([2., 2.]), tensor([2., 2.]), tensor([2.5000, 2.0000]), tensor([2.5000, 2.0000])]\n","\n","backsub_matrices_lb:\n","tensor([-1., -1.,  1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 1.]])\n","\n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 1.]])\n","\n","\n","\n","backsub_matrices_ub:\n","tensor([1., 1., 1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0.5000, 0.0000, 1.0000],\n","        [0.0000, 0.5000, 1.0000],\n","        [0.0000, 0.0000, 1.0000]])\n","\n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","tensor([[0.8333, 0.0000, 0.4167],\n","        [0.0000, 0.5000, 1.0000],\n","        [0.0000, 0.0000, 1.0000]])\n","\n","\n","----------------------------------------------------------------------------------------\n","\n","Layer 5\n","\n","clb_list:\n","[tensor([-1., -1.]), tensor([-2., -2.]), tensor([0., 0.]), tensor([-0.5000, -2.0000]), tensor([0., 0.]), tensor([0.5000, 0.0000])]\n","\n","cub_list:\n","[tensor([1., 1.]), tensor([2., 2.]), tensor([2., 2.]), tensor([2.5000, 2.0000]), tensor([2.5000, 2.0000]), tensor([5., 2.])]\n","\n","backsub_matrices_lb:\n","tensor([-1., -1.,  1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 1.]])\n","\n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 1.]])\n","\n","tensor([[-1.,  1.,  3.],\n","        [ 0.,  1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","\n","\n","backsub_matrices_ub:\n","tensor([1., 1., 1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0.5000, 0.0000, 1.0000],\n","        [0.0000, 0.5000, 1.0000],\n","        [0.0000, 0.0000, 1.0000]])\n","\n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","tensor([[0.8333, 0.0000, 0.4167],\n","        [0.0000, 0.5000, 1.0000],\n","        [0.0000, 0.0000, 1.0000]])\n","\n","tensor([[-1.,  1.,  3.],\n","        [ 0.,  1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","\n","----------------------------------------------------------------------------------------\n","\n","Layer 6\n","\n","clb_list:\n","[tensor([-1., -1.]), tensor([-2., -2.]), tensor([0., 0.]), tensor([-0.5000, -2.0000]), tensor([0., 0.]), tensor([0.5000, 0.0000]), tensor([0.5000])]\n","\n","cub_list:\n","[tensor([1., 1.]), tensor([2., 2.]), tensor([2., 2.]), tensor([2.5000, 2.0000]), tensor([2.5000, 2.0000]), tensor([5., 2.]), tensor([3.])]\n","\n","backsub_matrices_lb:\n","tensor([-1., -1.,  1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 1.]])\n","\n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 1.]])\n","\n","tensor([[-1.,  1.,  3.],\n","        [ 0.,  1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","\n","\n","backsub_matrices_ub:\n","tensor([1., 1., 1.])\n","\n","tensor([[ 1.,  1.,  0.],\n","        [ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[0.5000, 0.0000, 1.0000],\n","        [0.0000, 0.5000, 1.0000],\n","        [0.0000, 0.0000, 1.0000]])\n","\n","tensor([[ 1.0000,  1.0000, -0.5000],\n","        [ 1.0000, -1.0000,  0.0000],\n","        [ 0.0000,  0.0000,  1.0000]])\n","\n","tensor([[0.8333, 0.0000, 0.4167],\n","        [0.0000, 0.5000, 1.0000],\n","        [0.0000, 0.0000, 1.0000]])\n","\n","tensor([[-1.,  1.,  3.],\n","        [ 0.,  1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","tensor([[ 1., -1.,  0.],\n","        [ 0.,  0.,  1.]])\n","\n","\n","----------------------------------------------------------------------------------------\n","\n","-----------------------------------forward completed-----------------------------------\n","\n","Final concrete lb:\n","tensor([0.5000])\n","\n","Total time: 0.1115729808807373 sec\n","\n","----------------------------------------------------------------------------------------\n","\n"]}]},{"cell_type":"code","source":["#just for curisosity, test the time for forward without debug prints\n","transformerNet = TransformerNet(net, verbose = False )\n","lb = torch.Tensor([-1., -1.])\n","ub = torch.Tensor([1., 1.])\n","start_time = time.time()\n","final_lb, final_ub = transformerNet(lb, ub)\n","print(\"Time without debug prints: {}\".format(time.time()-start_time))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qy10uCdpnpiK","executionInfo":{"status":"ok","timestamp":1700515628785,"user_tz":-60,"elapsed":317,"user":{"displayName":"Samuel Cestola","userId":"06050624223395700455"}},"outputId":"13a5f97e-4e9f-4f90-cbb3-c80b6bc4beb2"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["Time without debug prints: 0.0024890899658203125\n"]}]}]}